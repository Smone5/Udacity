{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create Redshift Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 1. Setup AWS Manual Way\n",
    "Intially setup your IAM roles, security groups, users, etc before doing it in a programmatic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### Create Amazon IAM role\n",
    "+ [Create an IAM role](https://console.aws.amazon.com/iam/home#/home)\n",
    "+ Ensure role has administrator access to redshift, ec2, s3 and other areas.\n",
    "                     \n",
    "##### Create Amazon Security Group\n",
    "+ [Create an Amazon Security Group](https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2#SecurityGroups:)\n",
    "+ Amazon Redshift needs a port range  = 5439\n",
    "\n",
    "##### Launch a Redshift Cluster\n",
    "+ [Launch Redshift Cluster](https://console.aws.amazon.com/redshift/)\n",
    "\n",
    "##### Create IAM User\n",
    "+ [Create IAM User](https://console.aws.amazon.com/iam/)\n",
    "+ Ensure user has programmatic access\n",
    "+ Attach policies for redshift, s3 and any othern necessary policies\n",
    "\n",
    "##### Create an S3 Bucket\n",
    "+ [Create an S3 Bucket](https://s3.console.aws.amazon.com/s3/home?region=us-west-2#)\n",
    "\n",
    "##### Create PostgreSQL RDS\n",
    "+ [Create a PostgreSQL RDS](https://us-west-2.console.aws.amazon.com/rds/home?region=us-west-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2. Setup AWS Programmatic Way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "After you have set up the initial AWS structure the manual way, you can create new Identity Access Management (IAM) Users using Python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create a configuration file called dwh.cfg file and insert parameters into file\n",
    "\n",
    "[AWS]\n",
    "KEY= \n",
    "SECRET=\n",
    "\n",
    "[CLUSTER]\n",
    "HOST=\n",
    "DB_NAME=\n",
    "DB_USER= \n",
    "DB_PASSWORD= \n",
    "DB_PORT= \n",
    "\n",
    "[IAM_ROLE]\n",
    "ARN=''\n",
    "\n",
    "[S3]\n",
    "LOG_DATA='s3://udacity-dend/log_data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# move up one directory for config files\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "from configparser import ConfigParser\n",
    "config = ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY=config.get('AWS','key')\n",
    "SECRET= config.get('AWS','secret')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create IAM Client\n",
    "\n",
    "from boto3 import client\n",
    "\n",
    "iam = client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name sparkify_redshift already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create IAM Role for Redshift to have ReadOnly access to S3\n",
    "# Important for Role to have high access like Administrator access\n",
    "\n",
    "from json import dumps\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "DB_ROLE_NAME = config.get(\"CLUSTER\", \"DB_ROLE_NAME\")\n",
    "\n",
    "try:\n",
    "    print(\"Creating new IAM Role\")\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName = DB_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=dumps(\n",
    "            {'Statement':[{'Action': 'sts:AssumeRole',\n",
    "                          'Effect':'Allow',\n",
    "                          'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                         'Version':'2012-10-17'})\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching Policy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach Necessary Policies to Role\n",
    "# Code is attaching the AmazonS3ReadOnlyAccess to Role\n",
    "\n",
    "print('Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=DB_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::341887061345:role/sparkify_redshift\n"
     ]
    }
   ],
   "source": [
    "# Get Amazon Resource Names (ARN) \n",
    "# The roleArn variable is used when creating a Redshift \n",
    "\n",
    "roleArn = iam.get_role(RoleName=DB_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Redshift client\n",
    "from boto3 import client\n",
    "\n",
    "redshift = client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data variables to create database from configuration file\n",
    "\n",
    "DB_CLUSTER_TYPE       = config.get(\"CLUSTER\",\"DB_CLUSTER_TYPE\")\n",
    "DB_NUM_NODES          = config.get(\"CLUSTER\",\"DB_NUM_NODES\")\n",
    "DB_NODE_TYPE          = config.get(\"CLUSTER\",\"DB_NODE_TYPE\")\n",
    "\n",
    "DB_HOST               = config.get(\"CLUSTER\",\"HOST\")\n",
    "DB_NAME               = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER               = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD           = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT               = config.get(\"CLUSTER\",\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Redshift Database\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # Parameters for hardware\n",
    "        ClusterType=DB_CLUSTER_TYPE,\n",
    "        NodeType=DB_NODE_TYPE,\n",
    "        NumberOfNodes=int(DB_NUM_NODES),\n",
    "        \n",
    "        # Parameters for identifiers & credentials\n",
    "        DBName=DB_NAME,\n",
    "        ClusterIdentifier=DB_HOST,\n",
    "        MasterUsername=DB_USER,\n",
    "        MasterUserPassword=DB_PASSWORD,\n",
    "        \n",
    "        # Parameter for role s3 access\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check on progess of creation and Redshift database type\n",
    "import pandas as pd\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DB_HOST)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check on the progress of the database being created every minute\n",
    "\n",
    "import time\n",
    "starttime=time.time()\n",
    "while (redshift.describe_clusters(ClusterIdentifier=DB_HOST)['Clusters'][0] == 'creating') == True:\n",
    "    print(\"Creating Redshift\")\n",
    "    time.sleep(60.0 - ((time.time() - starttime) % 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save DB Endpoint and DB ROLE ARN to configuration file\n",
    "DB_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DB_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "print(\"DWH_ENDPOINT :: \", DB_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DB_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create EC2 Resource\n",
    "from boto3 import resource\n",
    "\n",
    "ec2 = resource('ec2',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Open incoming TCP Port to Access Endpoint if Not Done Already\n",
    "\n",
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName= defaultSg.group_name,  # TODO: fill out\n",
    "        CidrIp='0.0.0.0/0',  # TODO: fill out\n",
    "        IpProtocol='TCP',  # TODO: fill out\n",
    "        FromPort=int(DB_PORT),\n",
    "        ToPort=int(DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# basic method not using psycopg2\n",
    "%load_ext sql\n",
    "\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, DB_PORT,DB_NAME)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
