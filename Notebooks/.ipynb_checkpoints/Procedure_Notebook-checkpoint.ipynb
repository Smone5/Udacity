{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Procedure Notebook\n",
    "This a notebook to explore and create processes before implementing in Python code. This is helpful for debugging and trying improve process before adjusting the production code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Overall Process Steps\n",
    "1. Create fact and dimension tables using a star schema for Redshift\n",
    "2. Load data into S3\n",
    "3. Load data from S3 into Redshift staging tables\n",
    "4. ETL data from Redshift staging tables to analytic tables\n",
    "5. Load data from Redshift into dashboard for analytics team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jupyter_nbextensions_configurator\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/a3/d72d5f2dc10c5ccf5a6f4c79f636bf071a5ce462dedd07af2f70384db6cb/jupyter_nbextensions_configurator-0.4.1.tar.gz (479kB)\n",
      "\u001b[K    100% |████████████████████████████████| 481kB 12.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jupyter_contrib_core>=0.3.3 (from jupyter_nbextensions_configurator)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/8f/04a752a8b66a66e7092c035e5d87d2502ac7ec07f9fb6059059b6c0dc272/jupyter_contrib_core-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jupyter_core in /opt/conda/lib/python3.6/site-packages (from jupyter_nbextensions_configurator) (4.4.0)\n",
      "Requirement already satisfied: notebook>=4.0 in /opt/conda/lib/python3.6/site-packages (from jupyter_nbextensions_configurator) (5.7.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from jupyter_nbextensions_configurator) (3.12)\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.6/site-packages (from jupyter_nbextensions_configurator) (4.5.3)\n",
      "Requirement already satisfied: traitlets in /opt/conda/lib/python3.6/site-packages (from jupyter_nbextensions_configurator) (4.3.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from jupyter_contrib_core>=0.3.3->jupyter_nbextensions_configurator) (38.4.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (2.10)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (17.1.2)\n",
      "Requirement already satisfied: ipython_genutils in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (0.2.0)\n",
      "Requirement already satisfied: jupyter_client>=5.2.0 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (5.2.4)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (4.4.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (5.4.0)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (4.9.0)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (0.8.1)\n",
      "Requirement already satisfied: prometheus_client in /opt/conda/lib/python3.6/site-packages (from notebook>=4.0->jupyter_nbextensions_configurator) (0.3.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.6/site-packages (from traitlets->jupyter_nbextensions_configurator) (4.0.11)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from traitlets->jupyter_nbextensions_configurator) (1.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from jinja2->notebook>=4.0->jupyter_nbextensions_configurator) (1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from jupyter_client>=5.2.0->notebook>=4.0->jupyter_nbextensions_configurator) (2.6.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat->notebook>=4.0->jupyter_nbextensions_configurator) (2.6.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.8.3)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (2.2.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.2.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (1.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (1.4.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.3.1)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.6/site-packages (from nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.5.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (6.5.0)\n",
      "Requirement already satisfied: html5lib!=0.9999,!=0.99999,<0.99999999,>=0.999 in /opt/conda/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.0->jupyter_nbextensions_configurator) (0.9999999)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (1.0.15)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.7.4)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (4.3.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.8.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.10.2)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->notebook>=4.0->jupyter_nbextensions_configurator) (0.5.2)\n",
      "Building wheels for collected packages: jupyter-nbextensions-configurator\n",
      "  Running setup.py bdist_wheel for jupyter-nbextensions-configurator ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/15/df/fe/2a74fe34709e7fdc5ae153a768675d9fda93cc7d5133ed1fb0\n",
      "Successfully built jupyter-nbextensions-configurator\n",
      "Installing collected packages: jupyter-contrib-core, jupyter-nbextensions-configurator\n",
      "Successfully installed jupyter-contrib-core-0.3.3 jupyter-nbextensions-configurator-0.4.1\n",
      "Enabling: jupyter_nbextensions_configurator\n",
      "- Writing config: /root/.jupyter\n",
      "    - Validating...\n",
      "      jupyter_nbextensions_configurator 0.4.1 \u001b[32mOK\u001b[0m\n",
      "Enabling notebook nbextension nbextensions_configurator/config_menu/main...\n",
      "Enabling tree nbextension nbextensions_configurator/tree_tab/main...\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter_nbextensions_configurator\n",
    "!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!pip install jupyter_contrib_nbextensions\n",
    "#!jupyter contrib nbextension install — user\n",
    "#!pip install jupyter_nbextensions_configurator\n",
    "#!jupyter nbextensions_configurator enable --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 1. Create fact and dimension tables using a star schema for Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 1. Setup AWS Manual Way\n",
    "Intially setup your IAM roles, security groups, users, etc before doing it in a programmatic way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### Create Amazon IAM role\n",
    "+ [Create an IAM role](https://console.aws.amazon.com/iam/home#/home)\n",
    "+ Ensure role has administrator access to redshift, ec2, s3 and other areas.\n",
    "                     \n",
    "##### Create Amazon Security Group\n",
    "+ [Create an Amazon Security Group](https://us-west-2.console.aws.amazon.com/ec2/v2/home?region=us-west-2#SecurityGroups:)\n",
    "+ Amazon Redshift needs a port range  = 5439\n",
    "\n",
    "##### Launch a Redshift Cluster\n",
    "+ [Launch Redshift Cluster](https://console.aws.amazon.com/redshift/)\n",
    "\n",
    "##### Create IAM User\n",
    "+ [Create IAM User](https://console.aws.amazon.com/iam/)\n",
    "+ Ensure user has programmatic access\n",
    "+ Attach policies for redshift, s3 and any othern necessary policies\n",
    "\n",
    "##### Create an S3 Bucket\n",
    "+ [Create an S3 Bucket](https://s3.console.aws.amazon.com/s3/home?region=us-west-2#)\n",
    "\n",
    "##### Create PostgreSQL RDS\n",
    "+ [Create a PostgreSQL RDS](https://us-west-2.console.aws.amazon.com/rds/home?region=us-west-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2. Programmatic Access Way to Add and Delete\n",
    "After you have set up the initial AWS structure the manual way, you can create new Identity Access Management (IAM) Users using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2a. Create a configuration file called dwh.cfg file and insert parameters into file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "[AWS]\n",
    "KEY= \n",
    "SECRET=\n",
    "\n",
    "[CLUSTER]\n",
    "HOST=\n",
    "DB_NAME=\n",
    "DB_USER= \n",
    "DB_PASSWORD= \n",
    "DB_PORT= \n",
    "\n",
    "[IAM_ROLE]\n",
    "ARN=''\n",
    "\n",
    "[S3]\n",
    "LOG_DATA='s3://udacity-dend/log_data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load configuration file\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY=config.get('AWS','key')\n",
    "SECRET= config.get('AWS','secret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2b. Create IAM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from boto3 import client\n",
    "\n",
    "iam = client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2c. Create IAM Role for Redshift to have ReadOnly access to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Important for Role to have high access like Administrator access\n",
    "\n",
    "from json import dumps\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "DB_ROLE_NAME = config.get(\"CLUSTER\", \"DB_ROLE_NAME\")\n",
    "\n",
    "try:\n",
    "    print(\"Creating new IAM Role\")\n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName = DB_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=dumps(\n",
    "            {'Statement':[{'Action': 'sts:AssumeRole',\n",
    "                          'Effect':'Allow',\n",
    "                          'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "                         'Version':'2012-10-17'})\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2d. Attach Necessary Policies to Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Code is attaching the AmazonS3ReadOnlyAccess to Role\n",
    "\n",
    "print('Attaching Policy')\n",
    "iam.attach_role_policy(RoleName=DB_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2e. Get Amazon Resource Names (ARN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#The roleArn variable is used when creating a Redshift \n",
    "roleArn = iam.get_role(RoleName=DB_ROLE_NAME)['Role']['Arn']\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2f. Create Amazon Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Redshift client\n",
    "from boto3 import client\n",
    "\n",
    "redshift = client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load data variables to create database from configuration file\n",
    "\n",
    "DB_CLUSTER_TYPE       = config.get(\"CLUSTER\",\"DB_CLUSTER_TYPE\")\n",
    "DB_NUM_NODES          = config.get(\"CLUSTER\",\"DB_NUM_NODES\")\n",
    "DB_NODE_TYPE          = config.get(\"CLUSTER\",\"DB_NODE_TYPE\")\n",
    "\n",
    "DB_HOST               = config.get(\"CLUSTER\",\"HOST\")\n",
    "DB_NAME               = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER               = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD           = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT               = config.get(\"CLUSTER\",\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Redshift Database\n",
    "\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        # Parameters for hardware\n",
    "        ClusterType=DB_CLUSTER_TYPE,\n",
    "        NodeType=DB_NODE_TYPE,\n",
    "        NumberOfNodes=int(DB_NUM_NODES),\n",
    "        \n",
    "        # Parameters for identifiers & credentials\n",
    "        DBName=DB_NAME,\n",
    "        ClusterIdentifier=DB_HOST,\n",
    "        MasterUsername=DB_USER,\n",
    "        MasterUserPassword=DB_PASSWORD,\n",
    "        \n",
    "        # Parameter for role s3 access\n",
    "        IamRoles=[roleArn]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check on progess of creation and Redshift database type\n",
    "import pandas as pd\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DB_HOST)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "starttime=time.time()\n",
    "while (redshift.describe_clusters(ClusterIdentifier=DB_HOST)['Clusters'][0] == 'creating') == True:\n",
    "    print(\"Creating Redshift\")\n",
    "    time.sleep(60.0 - ((time.time() - starttime) % 60.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DB_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DB_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2g. Save DB Endpoint and DB ROLE ARN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DB_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DB_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "\n",
    "print(\"DWH_ENDPOINT :: \", DB_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DB_ROLE_ARN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2h. Open incoming TCP Port to Access Endpoint if Not Done Already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create EC2 Resource\n",
    "from boto3 import resource\n",
    "\n",
    "ec2 = resource('ec2',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName= defaultSg.group_name,  # TODO: fill out\n",
    "        CidrIp='0.0.0.0/0',  # TODO: fill out\n",
    "        IpProtocol='TCP',  # TODO: fill out\n",
    "        FromPort=int(DB_PORT),\n",
    "        ToPort=int(DB_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### 2i. Verifiy Connection to Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# more basic method not using psycopg2 if necessary\n",
    "%load_ext sql\n",
    "\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DB_USER, DB_PASSWORD, DB_ENDPOINT, DB_PORT,DB_NAME)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Library for viewing the data from S3 in pandas\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3. Explore S3 Staging Data\n",
    "Get familiar with data used in S3 to create the schema for the SQL tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Song Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_path = 's3://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json'\n",
    "song_df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_path = 's3://udacity-dend/log_data/2018/11/2018-11-12-events.json'\n",
    "log_df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4. Create Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Staging Tables Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/staging_tables1.png\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Star Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/star_schema4.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "#If database is already created, enter information below. Otherwise complete steps 2a-2i above\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY=config.get('AWS','key')\n",
    "SECRET= config.get('AWS','secret')\n",
    "\n",
    "DB_NAME = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "\n",
    "DB_ROLE_ARN=\"\"\n",
    "DB_ENDPOINT=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "##To Do\n",
    "# Add references to tables\n",
    "# Add sorting keys and optimization for redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "CREATE SCHEMA IF NOT EXISTS sparkify;\n",
    "SET search_path TO sparkify;\n",
    "\n",
    "DROP TABLE IF EXISTS event_stage;\n",
    "DROP TABLE IF EXISTS song_stage;\n",
    "DROP TABLE IF EXISTS fact_songplay;\n",
    "DROP TABLE IF EXISTS dim_user;\n",
    "DROP TABLE IF EXISTS dim_song;\n",
    "DROP TABLE IF EXISTS dim_artist;\n",
    "DROP TABLE IF EXISTS dim_time;\n",
    "\n",
    "CREATE TABLE \"event_stage\" (\n",
    "  \"artist\" varchar,\n",
    "  \"auth\" varchar,\n",
    "  \"firstName\" varchar,\n",
    "  \"gender\" varchar(4),\n",
    "  \"itemInSession\" varchar,\n",
    "  \"lastName\" varchar,\n",
    "  \"length\" varchar,\n",
    "  \"level\" varchar,\n",
    "  \"location\" varchar,\n",
    "  \"method\" varchar,\n",
    "  \"page\" varchar,\n",
    "  \"registration\" varchar,\n",
    "  \"sessionId\" varchar,\n",
    "  \"song\" varchar,\n",
    "  \"status\" varchar,\n",
    "  \"ts\" varchar,\n",
    "  \"userAgent\" varchar,\n",
    "  \"userId\" varchar\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE \"song_stage\" (\n",
    "  \"artist_id\" varchar PRIMARY KEY,\n",
    "  \"artist_latitude\" varchar,\n",
    "  \"artist_location\" varchar,\n",
    "  \"artist_longitude\" varchar,\n",
    "  \"artist_name\" varchar,\n",
    "  \"duration\" varchar,\n",
    "  \"num_songs\" varchar,\n",
    "  \"song_id\" varchar,\n",
    "  \"title\" varchar,\n",
    "  \"year\" varchar\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE \"fact_songplay\" (\n",
    "  \"songplay_id\" varchar PRIMARY KEY,\n",
    "  \"start_time\" bigint NOT NULL,\n",
    "  \"user_id\" int NOT NULL,\n",
    "  \"level\" varchar NOT NULL,\n",
    "  \"song_id\" varchar,\n",
    "  \"artist_id\" varchar,\n",
    "  \"session_id\" int,\n",
    "  \"location\" varchar,\n",
    "  \"user_agent\" varchar\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE \"dim_user\" (\n",
    "  \"user_id\" int PRIMARY KEY NOT NULL,\n",
    "  \"first_name\" varchar,\n",
    "  \"last_name\" varchar,\n",
    "  \"gender\" varchar,\n",
    "  \"level\" varchar NOT NULL\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE \"dim_song\" (\n",
    "  \"song_id\" varchar PRIMARY KEY NOT NULL,\n",
    "  \"title\" varchar NOT NULL,\n",
    "  \"artist_id\" varchar,\n",
    "  \"year\" int,\n",
    "  \"duration\" float8\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE \"dim_artist\" (\n",
    "  \"artist_id\" varchar PRIMARY KEY NOT NULL,\n",
    "  \"name\" varchar NOT NULL,\n",
    "  \"location\" varchar,\n",
    "  \"latitude\" float8,\n",
    "  \"longitude\" float8\n",
    ");\n",
    "\n",
    "\n",
    "CREATE TABLE \"dim_time\" (\n",
    "  \"time_key\" bigint PRIMARY KEY NOT NULL,  \n",
    "  \"start_time\" timestamp NOT NULL,\n",
    "  \"hour\" int NOT NULL,\n",
    "  \"day\" int NOT NULL,\n",
    "  \"week\" int NOT NULL,\n",
    "  \"month\" int NOT NULL,\n",
    "  \"year\" int NOT NULL,\n",
    "  \"weekday\" varchar NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2. Load data into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# !pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load configuration file\n",
    "\n",
    "from configparser import ConfigParser\n",
    "\n",
    "config = ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY=config.get('AWS','key')\n",
    "SECRET= config.get('AWS','secret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This was already done by Udacity\n",
    "+ log data path: s3://udacity-dend/log_data\n",
    "    + song_data/A/B/C/TRABCEI128F424C983.json\n",
    "    + song_data/A/A/B/TRAABJL12903CDCF1A.json\n",
    "+ log data json pat\n",
    "    + s3://udacity-dend/log_json_path.json\n",
    "+ song data path: s3://udacity-dend/song_data\n",
    "    + log_data/2018/11/2018-11-12-events.json\n",
    "    + log_data/2018/11/2018-11-13-events.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from boto3 import resource\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# View log files\n",
    "from os import path\n",
    "log_bucket = s3.Bucket(name=\"udacity-dend\")\n",
    "\n",
    "log_bucket_list = []\n",
    "for obj in log_bucket.objects.filter(Prefix=\"log_data\"):\n",
    "    file_path = path.join(obj.bucket_name, obj.key)\n",
    "    log_bucket_list.append(file_path)\n",
    "    \n",
    "print(\"File count: \", len(log_bucket_list))\n",
    "print(log_bucket_list[1:3])\n",
    "print(log_bucket_list[-3:])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "##CAUTION!!! THIS TAKES SOME TIME\n",
    "# View song files\n",
    "from os import path\n",
    "song_bucket = s3.Bucket(name=\"udacity-dend\")\n",
    "\n",
    "song_bucket_list = []\n",
    "for obj in song_bucket.objects.filter(Prefix=\"song-data\"):\n",
    "    file_path = path.join(obj.bucket_name, obj.key)\n",
    "    song_bucket_list.append(file_path)\n",
    "   \n",
    "print(\"File count: \", len(song_bucket_list))\n",
    "print(song_bucket_list[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Uncomment if you need to view the data again\n",
    "log_df = pd.read_json('s3://udacity-dend/log_data/2018/11/2018-11-01-events.json', lines=True)\n",
    "log_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Uncomment if you need to view the data again\n",
    "song_df = pd.read_json('s3://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json', lines=True)\n",
    "song_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "jsonpath_df = pd.read_json('s3://udacity-dend/log_json_path.json')\n",
    "jsonpath_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3. Load data from S3 into Redshift staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "staging_events_copy = (\"\"\"\n",
    "copy event_stage\n",
    "from 's3://udacity-dend/log_data/2018/11/2018'\n",
    "credentials 'aws_iam_role={}'\n",
    "format as json 's3://udacity-dend/log_json_path.json';\n",
    "\"\"\").format(DB_ROLE_ARN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(staging_events_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $staging_events_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "SELECT count(*)\n",
    "FROM event_stage;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "SELECT *\n",
    "FROM event_stage\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "staging_songs_copy = (\"\"\"\n",
    "copy song_stage\n",
    "from 's3://udacity-dend/song_data/'\n",
    "credentials 'aws_iam_role={}'\n",
    "format as json 'auto';\n",
    "\"\"\").format(DB_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(staging_songs_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $staging_songs_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "SELECT count(*)\n",
    "FROM song_stage;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "SELECT *\n",
    "FROM song_stage\n",
    "limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4. ETL data from Redshift staging tables to analytic tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_user table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#If need to drop and re-load dim_user for testing convert"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dim_user"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE \"dim_user\" (\n",
    "  \"user_id\" int PRIMARY KEY NOT NULL,\n",
    "  \"first_name\" varchar,\n",
    "  \"last_name\" varchar,\n",
    "  \"gender\" varchar,\n",
    "  \"level\" varchar NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create dim_user table\n",
    "#user_id, first_name, last_name, gender, level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "\n",
    "INSERT INTO dim_user (user_id, first_name, last_name, gender, level)\n",
    "SELECT DISTINCT\n",
    "    CAST(e.userID as integer) AS user_id,\n",
    "    e.firstName AS first_name,\n",
    "    e.lastName AS last_name,\n",
    "    e.gender AS gender,\n",
    "    e.level AS level\n",
    "FROM event_stage e\n",
    "WHERE e.page = 'NextSong'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#check for duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT user_id, first_name, last_name, COUNT(*)\n",
    "FROM dim_user\n",
    "GROUP BY user_id, first_name, last_name\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM dim_user\n",
    "WHERE user_id = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note**: Some users have a free and paid level that needs to be used for filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_song table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#If need to drop and re-load dim_song for testing convert"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dim_song"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "%%sql\n",
    "CREATE TABLE \"dim_artist\" (\n",
    "  \"song_id\" varchar PRIMARY KEY NOT NULL,\n",
    "  \"title\" varchar NOT NULL,\n",
    "  \"artist_id\" varchar,\n",
    "  \"year\" int,\n",
    "  \"duration\" float8\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create dim_song table\n",
    "#song_id, title, artist_id, year, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "INSERT INTO dim_song (song_id, title, artist_id, year, duration)\n",
    "SELECT DISTINCT\n",
    "    s.song_id AS song_id,\n",
    "    s.title AS title,\n",
    "    s.artist_id AS artist_id,\n",
    "    CAST(s.year as integer) AS year,\n",
    "    CAST(s.duration as decimal(8,2)) AS duration\n",
    "FROM song_stage s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# View first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM dim_song\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check for dupilcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    s.song_id,\n",
    "    s.title,\n",
    "    COUNT(*)\n",
    "FROM dim_song s\n",
    "GROUP BY s.song_id, s.title\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_artist table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#If need to drop and re-load dim_artist for testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dim_artist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "editable": true
   },
   "source": [
    "%%sql\n",
    "CREATE TABLE \"dim_artist\" (\n",
    "  \"artist_id\" varchar PRIMARY KEY NOT NULL,\n",
    "  \"name\" varchar NOT NULL,\n",
    "  \"location\" varchar,\n",
    "  \"latitude\" float8,\n",
    "  \"longitude\" float8\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create dim_artist\n",
    "#artist_id, name, location, latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "\n",
    "INSERT INTO dim_artist (artist_id, name, location, latitude, longitude)\n",
    "SELECT DISTINCT\n",
    "    s.artist_id AS artist_id,\n",
    "    s.artist_name AS name,\n",
    "    s.artist_location AS location,\n",
    "    CONVERT(float, s.artist_latitude) AS latitude,\n",
    "    CONVERT(float, s.artist_longitude) AS longitude\n",
    "FROM song_stage s\n",
    "JOIN event_stage e\n",
    "    ON (e.artist = s.artist_name AND e.song = s.title)\n",
    "WHERE e.page = 'NextSong'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM dim_artist\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT\n",
    "    a.artist_id,\n",
    "    a.name,\n",
    "    COUNT(*)\n",
    "FROM dim_artist a\n",
    "GROUP BY a.artist_id, a.name\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM dim_artist a\n",
    "WHERE a.name = 'Dwight Yoakam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create dim_time\n",
    "#time_key, start_time, hour, day, week, month, year, weekday\n",
    "#NOTE!!!, see if can ingest ts as int to stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### dim_time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "CREATE TABLE \"dim_time\" (\n",
    "  \"time_key\" bigint PRIMARY KEY NOT NULL,  \n",
    "  \"start_time\" timestamp NOT NULL,\n",
    "  \"hour\" int NOT NULL,\n",
    "  \"day\" int NOT NULL,\n",
    "  \"week\" int NOT NULL,\n",
    "  \"month\" int NOT NULL,\n",
    "  \"year\" int NOT NULL,\n",
    "  \"weekday\" varchar NOT NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "INSERT INTO dim_time(time_key, start_time, hour, day, week, month, year, weekday)\n",
    "SELECT DISTINCT\n",
    "    CAST(e.ts as bigint) AS time_key,\n",
    "    TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second' as start_time,\n",
    "    EXTRACT(hour from TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second') AS hour,\n",
    "    CAST(DATE_PART(day, TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second')  as Integer) AS day,\n",
    "    CAST(DATE_PART(week, TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second') as Integer) AS week,\n",
    "    CAST(DATE_PART(month, TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second') as Integer) AS month,\n",
    "    CAST(DATE_PART(year, TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second') as Integer) AS year,\n",
    "    CASE\n",
    "        WHEN(\n",
    "                DATE_PART(dayofweek, TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second') = 0.0\n",
    "                OR\n",
    "                DATE_PART(dayofweek, TIMESTAMP 'epoch' + e.ts/1000 *INTERVAL '1 second') = 6.0\n",
    "            )\n",
    "        THEN 'no'\n",
    "        ELSE 'yes'\n",
    "        END\n",
    "        AS weekday\n",
    "FROM event_stage e\n",
    "WHERE e.page = 'NextSong'\n",
    "ORDER BY time_key ASC;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# view first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "SELECT *\n",
    "FROM dim_time\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "SELECT\n",
    "    t.time_key,\n",
    "    COUNT(*)\n",
    "FROM dim_time t\n",
    "GROUP BY t.time_key\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### fact_songplay table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#time_key, user_id, level, song_id, artist_id, session_id, location, user_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Note**: \n",
    "+ Many songs listed in events table don't have a song in songs table. Like you \"You Gotta Be\" from Des'ree. That is because the data is only a subset of 14,896 rows of the 1 million song database. So it is ok if the simulated events don't have the song sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM song_stage\n",
    "WHERE title like 'You Gotta Be'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT Count(*)\n",
    "FROM song_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#1541106106796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS \"fact_songplay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE \"fact_songplay\" (\n",
    "  \"songplay_id\" varchar PRIMARY KEY,\n",
    "  \"start_time\" bigint NOT NULL,\n",
    "  \"user_id\" int NOT NULL,\n",
    "  \"level\" varchar NOT NULL,\n",
    "  \"song_id\" varchar,\n",
    "  \"artist_id\" varchar,\n",
    "  \"session_id\" int,\n",
    "  \"location\" varchar,\n",
    "  \"user_agent\" varchar\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "INSERT INTO fact_songplay(songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "SELECT\n",
    "    e.userId || st.song_id || e.itemInSession as songplay_id,\n",
    "    CAST(e.ts as bigint) as start_time,\n",
    "    CAST(e.userId as int) as user_id,\n",
    "    e.level as level,\n",
    "    st.song_id,\n",
    "    st.artist_id,\n",
    "    CAST(e.itemInSession as int) as session_id,\n",
    "    e.location as location,\n",
    "    e.userAgent as user_agent\n",
    "FROM (select * from event_stage where page = 'NextSong') as e\n",
    "LEFT JOIN song_stage st\n",
    "    ON (e.artist = st.artist_name OR e.song = st.title)\n",
    "WHERE song_id <> 'None'\n",
    "ORDER BY start_time ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO sparkify;\n",
    "SELECT *\n",
    "FROM fact_songplay\n",
    "LIMIT 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Quality Checks using psycop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "host = config['CLUSTER']['DB_ENDPOINT']\n",
    "dbname = config['CLUSTER']['DB_NAME']\n",
    "user = config['CLUSTER']['DB_USER']\n",
    "password = config['CLUSTER']['DB_PASSWORD']\n",
    "port = config['CLUSTER']['DB_PORT']\n",
    "\n",
    "psy_conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(host, dbname, user, password, port))\n",
    "psy_cur = psy_conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_table_dups = (\"\"\"\n",
    "SELECT user_id, first_name, last_name, level, COUNT(*)\n",
    "FROM sparkify.dim_user\n",
    "GROUP BY user_id, first_name, last_name, level\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 5\n",
    ";\n",
    "\"\"\")\n",
    "song_table_dups = (\"\"\"\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "artist_table_dups =(\"\"\"\n",
    "SELECT\n",
    "    a.artist_id,\n",
    "    a.name,\n",
    "    COUNT(*)\n",
    "FROM dim_artist a\n",
    "GROUP BY a.artist_id, a.name\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;\n",
    "\"\"\")\n",
    "\n",
    "time_table_dups = (\"\"\"\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_duplicates(query, conn, table):\n",
    "\n",
    "    count_df = pd.read_sql(query, conn)\n",
    "    count_s = count_df['count']\n",
    "    total_count = 0\n",
    "    for val in count_s:\n",
    "        if val == 1:\n",
    "            total_count += 0\n",
    "        \n",
    "    if total_count > 0:\n",
    "        return(print(\"Rows duplicated in table {}\".format(table)))\n",
    "        \n",
    "    else:\n",
    "        return(print(\"No rows duplicated in table {}\".format(table)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "check_duplicates(artist_table_dups, psy_conn, table='artist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_df = pd.read_sql(user_table_dups, psy_conn)\n",
    "\n",
    "count_s = count_df['count']\n",
    "query = 'user_table_dups'\n",
    "total_count = 0\n",
    "for val in count_s:\n",
    "    if val == 1:\n",
    "        total_count += 0\n",
    "        \n",
    "        \n",
    "if total_count > 0:\n",
    "    print(\"Rows duplicated in {}\".format(query))\n",
    "else:\n",
    "    print(\"No rows duplicated in {}\".format(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "psy_cur.execute(user_table_dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Delete Resources Once Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# delete Redshift resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DB_HOST,  SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# verify deletion\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DB_HOST)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#remove roles and policies\n",
    "iam.detach_role_policy(RoleName=DB_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=DB_ROLE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
